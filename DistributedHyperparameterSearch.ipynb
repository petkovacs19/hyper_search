{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a demonstration of distributed search/optimization of hyperparameters. \n",
    "If you had several GPUs available you might have been wondering how can you evaluate your model with different parameters in a distributed fashion without having to couple and highly customize your code to the infrastructure. \n",
    "\n",
    "The parameters you are looking for can be anything from learning rate to number of layers or channels in certain layers. \n",
    "Evaluating your model with different parameters may give you a better understanding on the performance of the model and how certain parameters affect your objectives.\n",
    "\n",
    "In this notebook we will use dask for distributing evaluations and a bayesian optimization loop to search for hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask is a library that provides capabilities to scale up your operations, and execute them parallel on a cluster. We will set up a scheduler and create several workers with different resources avaiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Start a dask scheduler**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing __[dask](http://docs.dask.org/en/latest/install.html)__ run the command below. This will start up a scheduler \n",
    "```\n",
    "dask-scheduler\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output the url of the scheduler where you need to register workers. Copy the url of the register and register several worker with different GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Register workers to the dask scheduler**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "CUDA_VISIBLE_DEVICES=0 dask-worker tcp://45.2.144.197:8786\n",
    "CUDA_VISIBLE_DEVICES=1 dask-worker tcp://45.2.144.197:8786\n",
    "CUDA_VISIBLE_DEVICES=2 dask-worker tcp://45.2.144.197:8786\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA_VISIBLE_DEVICES will set the available GPU for the given workers. Once the workers are registered you can monitor them using the bokeh. The url of the bokeh is output when you started the scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bokeh](bokeh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have three workers available each of them with a dedicated GPU. Time to fire up our optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skopt\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import Optimizer\n",
    "from skopt.learning import GaussianProcessRegressor\n",
    "from skopt.space import Real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skopt provides an optimization interface that runs a Bayesian optimization loop underneath. The 'ask and tell' api of Skopt at each evaluation cycle gives a set of hyperparameters that will likely to perform better based on previous observations. For more information see __[Ask and Tell](https://scikit-optimize.github.io/notebooks/ask-and-tell.html)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps are the following:\n",
    "0. You define the ranges of the parameters you are looking for\n",
    "1. You **ask** for x(number of workers) sets of hyperparameters\n",
    "2. Evaluate the performance of your model using the hyperparameters on the workers\n",
    "3. **Tell** the optimizer the results of the evaluations\n",
    "4. Repeat the process from 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Define the ranges of parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rate_range = Real(low=float(1e-6), high=float(1e+1), prior='log-uniform', name = 'learning_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keep prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob_range = Real(low=0.1, high=1.0, prior='log-uniform', name = 'keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Channel range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_range = Integer(low=3, high=7, name='channel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pool range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_range = Categorical(categories=['max', 'avg'], name='pool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [lr_rate_range, keep_prob_range, channel_range, pool_range]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation function"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Define your evaluation function. In case a deep learning model you can use a number of epoch to see how your model performs. Of course you have to take into consideration certain parameters may impact your loss in fixed amount of epochs but may perform better if you train your network longer.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_parameters(learning_rate, keep_prob, channel, pool):\n",
    "    #Evaluate your model for a few epochs and return loss value\n",
    "    print(\"Lr {} kp {} channel {} pool {}\".format(learning_rate, keep_prob, channel, pool))\n",
    "    loss = 0.4\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Optimizer(dimensions=dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Register scheduler and kick off optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed.joblib import DistributedBackend \n",
    "from sklearn.externals.joblib import Parallel, parallel_backend, register_parallel_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_OF_WORKERS = 3\n",
    "SCHEDULER_URL = 'tcp://45.2.144.197:8786' ## url of the scheduler\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_parallel_backend('distributed', DistributedBackend)\n",
    "with parallel_backend('distributed', scheduler_host=SCHEDULER_URL):\n",
    "    for i in tqdm(range(0,iterations)):\n",
    "        x = optimizer.ask(n_points=NO_OF_WORKERS)\n",
    "        y = Parallel()(delayed(evaluate_parameters)(float(p[0]),float(p[1]),int(p[2]),p[3]) for p in x)\n",
    "        optimizer.tell(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will evaluate NO_OF_WORKERS sets of parameters in parallel and when finished tells the optimizer the result and ask for the next sets of parameters. It is important to note that if runtime depends on the values of the hyperparamers, this is not efficient, as workers that finish earlier will be blocked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Visualise your results](https://scikit-optimize.github.io/notebooks/visualizing-results.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
